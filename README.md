# Tokenization module

This module provides custom tokenizers for the second laboratory of computational linguistics classes.

## Tokenizers

- `WhitespaceTokenizer`: A simple tokenizer that splits text based on whitespace and punctuation.
- `BocianTokenizer`: A BPE-based tokenizer.

Both tokenizers were trained on a Polish poetry corpus (`plwikisource` dataset from the SpeakLeash library).
